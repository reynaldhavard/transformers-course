{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84753da5-4952-4e0d-b61a-61641a38f67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reynald/transformers-course/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c08261-bafd-4815-ae55-71859a8818f8",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f31d735-b947-4882-9f29-5f43cf6093a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598048329353333}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "557db9bb-2f9c-4369-8fd0-b598c74034f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598048329353333},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a819f17c-b938-445b-bf2e-b3b1419f49d2",
   "metadata": {},
   "source": [
    "# Zero-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af48e009-fb25-428e-adb8-a94120d80796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445989489555359, 0.11197412759065628, 0.04342695698142052]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30d087a2-147d-47ee-ac34-8a00281968f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I am currently learning about NLP to improve my skills.',\n",
       " 'labels': ['work', 'education', 'lifestyle', 'sport', 'music'],\n",
       " 'scores': [0.5118930339813232,\n",
       "  0.31942567229270935,\n",
       "  0.11411619186401367,\n",
       "  0.032750096172094345,\n",
       "  0.02181488834321499]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    \"I am currently learning about NLP to improve my skills.\",\n",
    "    candidate_labels=[\"education\", \"music\", \"work\", \"lifestyle\", \"sport\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2665b-72be-4cd5-a721-23fbe28204cb",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bede4e6-3598-4056-98ed-5255b05cebd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/reynald/transformers-course/.env/lib/python3.10/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to make your own custom dice games like The World of Chess and Star Wars Battlefront, so you can easily make the most of them!\\n\\nOnce again, we encourage you to get started with the'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b8bc3e-c4c8-46ea-b9f2-760e9aa713c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Once upon a time, most people thought that a young woman of only eighteen years of age and a member of an impoverished, disunited group could save a man in a moment of utter violence.\\n\\nIn her second attempt, she destroyed the window'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Once upon a time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eddafc-4302-469b-87dd-dd8451345216",
   "metadata": {},
   "source": [
    "### Using any model from the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45a5050a-3a68-4e56-b024-5fdd7b81b55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"In this course, we will teach you how to code at Microsoft Azure, and we'll be teaching you how to be an expert who is not only\"},\n",
       " {'generated_text': \"In this course, we will teach you how to start writing online projects and learn more about how to create beautiful projects. In this book, we'll\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b53ac5ff-c97c-4336-a2d7-875a85a65375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Once upon a time, the average citizen of California has the most recent national population, with over 5 million people, with up to 1.1 million households per household.\\n\\n\\n\\nThe fact that Californians aren't even getting to know about the most recent national census is a big plus. With more than 4 million people in the U.S., these data will undoubtedly become essential to our well-being.\"},\n",
       " {'generated_text': 'Once upon a time when we did not have a whole lot to go through, the problem is our culture needs to take hold of us. As the United States prepares for the Fourth Asian Century, we will be able to work together effectively to protect and serve our country, and not just when the conflict would be between the Philippines and the West, or the Chinese over a long period of time.\\n\\nThe current situation needs remember that we also have not only no choice but to confront the situation in'},\n",
       " {'generated_text': 'Once upon a time, we are no longer able to take advantage of what we do. That can have great, devastating consequences, which is why it should be taken up or considered by other authorities and international organizations to address these concerns and ensure no one has even taken advantage of what we have done to keep the United States out of the world.\\n\\n\\nWe will be calling upon the United States to act decisively in addressing the financial crisis involving the debt ceiling vote. We will be calling upon the'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Once upon a time\", max_length=100, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525cc4f2-b86e-41a8-8f92-e479d45a8c2c",
   "metadata": {},
   "source": [
    "# Mask filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f709f5b7-d67e-4226-bb21-aafd750c33e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19619806110858917,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models.'},\n",
       " {'score': 0.04052723944187164,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you all about computational models.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "129be5ab-4999-4421-b665-e1bd1b709fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88852fe8-2aa1-4fde-a3eb-2536362a9c41",
   "metadata": {},
   "outputs": [
    {
     "ename": "PipelineException",
     "evalue": "No mask_token ([MASK]) found on the input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPipelineException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43munmasker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThis course will teach you all about <mask> models.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#wrong mask token\u001b[39;00m\n",
      "File \u001b[0;32m~/transformers-course/.env/lib/python3.10/site-packages/transformers/pipelines/fill_mask.py:239\u001b[0m, in \u001b[0;36mFillMaskPipeline.__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m    Fill the masked token in the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m        - **token_str** (`str`) -- The predicted token (to replace the masked one).\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/transformers-course/.env/lib/python3.10/site-packages/transformers/pipelines/base.py:1119\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1112\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1113\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         )\n\u001b[1;32m   1117\u001b[0m     )\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/transformers-course/.env/lib/python3.10/site-packages/transformers/pipelines/base.py:1125\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1125\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1127\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m~/transformers-course/.env/lib/python3.10/site-packages/transformers/pipelines/fill_mask.py:97\u001b[0m, in \u001b[0;36mFillMaskPipeline.preprocess\u001b[0;34m(self, inputs, return_tensors, **preprocess_parameters)\u001b[0m\n\u001b[1;32m     95\u001b[0m     return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework\n\u001b[1;32m     96\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(inputs, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors)\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_exactly_one_mask_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_inputs\n",
      "File \u001b[0;32m~/transformers-course/.env/lib/python3.10/site-packages/transformers/pipelines/fill_mask.py:91\u001b[0m, in \u001b[0;36mFillMaskPipeline.ensure_exactly_one_mask_token\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 91\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_exactly_one_mask_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/transformers-course/.env/lib/python3.10/site-packages/transformers/pipelines/fill_mask.py:79\u001b[0m, in \u001b[0;36mFillMaskPipeline._ensure_exactly_one_mask_token\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     77\u001b[0m numel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mprod(masked_index\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numel \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineException(\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfill-mask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbase_model_prefix,\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo mask_token (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmask_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) found on the input\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     83\u001b[0m     )\n",
      "\u001b[0;31mPipelineException\u001b[0m: No mask_token ([MASK]) found on the input"
     ]
    }
   ],
   "source": [
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2) #wrong mask token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edb31024-90a9-4331-9729-a921d10e8d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.25963184237480164,\n",
       "  'token': 1648,\n",
       "  'token_str': 'role',\n",
       "  'sequence': 'This course will teach you all about role models.'},\n",
       " {'score': 0.09427252411842346,\n",
       "  'token': 1103,\n",
       "  'token_str': 'the',\n",
       "  'sequence': 'This course will teach you all about the models.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"This course will teach you all about [MASK] models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d8417-0217-4a05-ab75-20531d3a2742",
   "metadata": {},
   "source": [
    "# Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2c2bc73-4b5d-4449-b3ff-0b761d72f1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/home/reynald/transformers-course/.env/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06f731b5-cc3c-40fd-8c80-9b02bf60840e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "392d9da1-b739-42a9-8f5d-4a93c4702eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/home/reynald/transformers-course/.env/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"none\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f943c58-e237-4bda-b7b4-f10b0e29d7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.99938285,\n",
       "  'index': 4,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99815494,\n",
       "  'index': 5,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99590707,\n",
       "  'index': 6,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99923277,\n",
       "  'index': 7,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9738931,\n",
       "  'index': 12,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.976115,\n",
       "  'index': 13,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9887976,\n",
       "  'index': 14,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9932106,\n",
       "  'index': 16,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba1ebbbc-d3b0-4e06-a42c-ed034d738813",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pipeline(\"ner\", model=\"vblagoje/bert-english-uncased-finetuned-pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "452022f9-4c14-4c64-adac-ad210e69a2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'PRON',\n",
       "  'score': 0.9994592,\n",
       "  'index': 1,\n",
       "  'word': 'my',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.99601364,\n",
       "  'index': 2,\n",
       "  'word': 'name',\n",
       "  'start': 3,\n",
       "  'end': 7},\n",
       " {'entity': 'AUX',\n",
       "  'score': 0.9953696,\n",
       "  'index': 3,\n",
       "  'word': 'is',\n",
       "  'start': 8,\n",
       "  'end': 10},\n",
       " {'entity': 'PROPN',\n",
       "  'score': 0.99848914,\n",
       "  'index': 4,\n",
       "  'word': 'sy',\n",
       "  'start': 11,\n",
       "  'end': 13},\n",
       " {'entity': 'PROPN',\n",
       "  'score': 0.9978808,\n",
       "  'index': 5,\n",
       "  'word': '##lva',\n",
       "  'start': 13,\n",
       "  'end': 16},\n",
       " {'entity': 'PROPN',\n",
       "  'score': 0.99808747,\n",
       "  'index': 6,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'CCONJ',\n",
       "  'score': 0.99918765,\n",
       "  'index': 7,\n",
       "  'word': 'and',\n",
       "  'start': 19,\n",
       "  'end': 22},\n",
       " {'entity': 'PRON',\n",
       "  'score': 0.9994679,\n",
       "  'index': 8,\n",
       "  'word': 'i',\n",
       "  'start': 23,\n",
       "  'end': 24},\n",
       " {'entity': 'VERB',\n",
       "  'score': 0.99923587,\n",
       "  'index': 9,\n",
       "  'word': 'work',\n",
       "  'start': 25,\n",
       "  'end': 29},\n",
       " {'entity': 'ADP',\n",
       "  'score': 0.9063106,\n",
       "  'index': 10,\n",
       "  'word': 'at',\n",
       "  'start': 30,\n",
       "  'end': 32},\n",
       " {'entity': 'PROPN',\n",
       "  'score': 0.7182434,\n",
       "  'index': 11,\n",
       "  'word': 'hugging',\n",
       "  'start': 33,\n",
       "  'end': 40},\n",
       " {'entity': 'PROPN',\n",
       "  'score': 0.7198605,\n",
       "  'index': 12,\n",
       "  'word': 'face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'ADP',\n",
       "  'score': 0.9993789,\n",
       "  'index': 13,\n",
       "  'word': 'in',\n",
       "  'start': 46,\n",
       "  'end': 48},\n",
       " {'entity': 'PROPN',\n",
       "  'score': 0.9989513,\n",
       "  'index': 14,\n",
       "  'word': 'brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57},\n",
       " {'entity': 'PUNCT',\n",
       "  'score': 0.99963903,\n",
       "  'index': 15,\n",
       "  'word': '.',\n",
       "  'start': 57,\n",
       "  'end': 58}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89e615f9-e117-4d25-bcc3-8dd1030b5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pipeline(\"ner\", model=\"vblagoje/bert-english-uncased-finetuned-pos\", grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f8407c0-ab61-4b99-9cc2-b326cceff15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PRON',\n",
       "  'score': 0.9994592,\n",
       "  'word': 'my',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': 0.99601364,\n",
       "  'word': 'name',\n",
       "  'start': 3,\n",
       "  'end': 7},\n",
       " {'entity_group': 'AUX',\n",
       "  'score': 0.9953696,\n",
       "  'word': 'is',\n",
       "  'start': 8,\n",
       "  'end': 10},\n",
       " {'entity_group': 'PROPN',\n",
       "  'score': 0.9981525,\n",
       "  'word': 'sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'CCONJ',\n",
       "  'score': 0.99918765,\n",
       "  'word': 'and',\n",
       "  'start': 19,\n",
       "  'end': 22},\n",
       " {'entity_group': 'PRON',\n",
       "  'score': 0.9994679,\n",
       "  'word': 'i',\n",
       "  'start': 23,\n",
       "  'end': 24},\n",
       " {'entity_group': 'VERB',\n",
       "  'score': 0.99923587,\n",
       "  'word': 'work',\n",
       "  'start': 25,\n",
       "  'end': 29},\n",
       " {'entity_group': 'ADP',\n",
       "  'score': 0.9063106,\n",
       "  'word': 'at',\n",
       "  'start': 30,\n",
       "  'end': 32},\n",
       " {'entity_group': 'PROPN',\n",
       "  'score': 0.71905196,\n",
       "  'word': 'hugging face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'ADP',\n",
       "  'score': 0.9993789,\n",
       "  'word': 'in',\n",
       "  'start': 46,\n",
       "  'end': 48},\n",
       " {'entity_group': 'PROPN',\n",
       "  'score': 0.9989513,\n",
       "  'word': 'brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57},\n",
       " {'entity_group': 'PUNCT',\n",
       "  'score': 0.99963903,\n",
       "  'word': '.',\n",
       "  'start': 57,\n",
       "  'end': 58}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47fd560-cd8a-42bf-8481-27cbc1f09f2d",
   "metadata": {},
   "source": [
    "# Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "617e1a99-1bdc-4c12-a564-dd390c431a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6949767470359802, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5785e39-a387-429e-9642-e568d1c0071f",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad226d98-2e44-4647-ad6d-14719631aea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd5528b-6b1b-424a-9180-c535270748b7",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2d543d6-5cd7-4ee9-bfc9-b209ff3f18c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reynald/transformers-course/.env/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by Hugging Face.'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e57cc15-1874-48f5-8114-ba7059e0004c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-course",
   "language": "python",
   "name": "transformers-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
